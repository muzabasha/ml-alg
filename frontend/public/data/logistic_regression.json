{
    "id": "logistic_regression",
    "name": "Logistic Regression",
    "category": "Supervised Learning - Classification",
    "difficulty": "Beginner",
    "estimatedTime": "50 minutes",
    "sections": {
        "introduction": {
            "title": "Introduction to Logistic Regression",
            "plainLanguage": "Logistic Regression is used when you need to predict yes/no, true/false, or categorical outcomes. Despite its name, it's a classification algorithm, not regression. It predicts the probability that something belongs to a particular category.",
            "realWorldAnalogy": "Think of a spam filter: given an email's features (words, sender, links), Logistic Regression calculates the probability it's spam. If probability > 50%, classify as spam; otherwise, not spam.",
            "whereAndWhy": "Used for binary classification: disease diagnosis (sick/healthy), customer churn (stay/leave), loan approval (approve/reject), email spam detection.",
            "learningType": "Supervised Learning (Classification)",
            "strengths": [
                "Outputs probabilities, not just class labels—tells you how confident the prediction is",
                "Simple, fast, and interpretable",
                "Works well with linearly separable data",
                "Less prone to overfitting than complex models"
            ],
            "limitations": [
                "Assumes linear decision boundary (can't handle complex patterns)",
                "Struggles with non-linear relationships",
                "Requires large sample size for stable estimates",
                "Sensitive to outliers"
            ]
        },
        "mathematical_model": {
            "title": "Mathematical Formulation",
            "introduction": "Logistic Regression uses the sigmoid function to map any real number to a probability between 0 and 1.",
            "equations": [
                {
                    "name": "Sigmoid Function",
                    "latex": "\\sigma(z) = \\frac{1}{1 + e^{-z}}",
                    "explanation": "Squashes any input z into range (0, 1). This gives us probabilities."
                },
                {
                    "name": "Hypothesis Function",
                    "latex": "h(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n)}}",
                    "explanation": "Predicts probability that y=1 given input x. θ are parameters we learn."
                },
                {
                    "name": "Cost Function (Log Loss)",
                    "latex": "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1-h(x^{(i)}))]",
                    "explanation": "Penalizes wrong predictions heavily. Minimizing this makes predictions more accurate."
                },
                {
                    "name": "Gradient Descent Update",
                    "latex": "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)}",
                    "explanation": "Same form as linear regression, but h(x) is now the sigmoid function."
                }
            ],
            "keyTerms": {
                "σ(z)": "Sigmoid function—converts any number to probability",
                "θ": "Parameters (weights) we learn",
                "h(x)": "Predicted probability that y=1",
                "y": "Actual class label (0 or 1)",
                "Log Loss": "Cost function for classification"
            },
            "intuition": "The sigmoid function is like a smooth step: very negative inputs → probability near 0; very positive inputs → probability near 1. The model learns weights that push correct examples toward their true class."
        },
        "sample_io": {
            "title": "Sample Input & Output",
            "description": "Predicting whether a student passes an exam based on hours studied.",
            "input": {
                "format": "Features and binary labels",
                "table": [
                    {
                        "Hours Studied": 1,
                        "Passed": 0
                    },
                    {
                        "Hours Studied": 2,
                        "Passed": 0
                    },
                    {
                        "Hours Studied": 3,
                        "Passed": 0
                    },
                    {
                        "Hours Studied": 4,
                        "Passed": 1
                    },
                    {
                        "Hours Studied": 5,
                        "Passed": 1
                    },
                    {
                        "Hours Studied": 6,
                        "Passed": 1
                    }
                ]
            },
            "output": {
                "parameters": {
                    "θ₀ (intercept)": -3.5,
                    "θ₁ (weight)": 1.2
                },
                "predictions": [
                    {
                        "Hours": 1,
                        "Probability": 0.05,
                        "Predicted Class": 0,
                        "Actual": 0
                    },
                    {
                        "Hours": 3,
                        "Probability": 0.27,
                        "Predicted Class": 0,
                        "Actual": 0
                    },
                    {
                        "Hours": 5,
                        "Probability": 0.82,
                        "Predicted Class": 1,
                        "Actual": 1
                    }
                ],
                "metrics": {
                    "Accuracy": 0.95,
                    "Precision": 0.93,
                    "Recall": 0.97
                }
            },
            "visualization": "Scatter plot with pass/fail points. S-shaped sigmoid curve shows decision boundary."
        },
        "implementation_scratch": {
            "title": "Python Implementation (From Scratch)",
            "description": "Building Logistic Regression using only NumPy.",
            "code": "import numpy as np\n\nclass LogisticRegressionFromScratch:\n    def __init__(self, learning_rate=0.01, iterations=1000):\n        self.lr = learning_rate\n        self.iterations = iterations\n        self.theta = None\n    \n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def fit(self, X, y):\n        m, n = X.shape\n        X = np.c_[np.ones(m), X]\n        self.theta = np.zeros(n + 1)\n        for _ in range(self.iterations):\n            h = self.sigmoid(X @ self.theta)\n            gradient = (1/m) * X.T @ (h - y)\n            self.theta -= self.lr * gradient\n        return self\n\n    def predict(self, X):\n        m = X.shape[0]\n        X = np.c_[np.ones(m), X]\n        return (self.sigmoid(X @ self.theta) >= 0.5).astype(int)"
        },
        "implementation_api": {
            "title": "Python Implementation (Using scikit-learn)",
            "description": "Production-ready implementation.",
            "code": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X, y)\npreds = model.predict(X)"
        },
        "evaluation": {
            "title": "Model Evaluation",
            "why": "Accuracy alone is misleading for imbalanced datasets.",
            "metrics": [
                {
                    "name": "Accuracy",
                    "formula": "Accuracy = (TP + TN) / Total",
                    "interpretation": "Percentage of correct predictions.",
                    "example": "95% accuracy"
                }
            ]
        },
        "improvements": {
            "title": "Ways to Improve Performance",
            "featureEngineering": [
                "Add polynomial features",
                "Feature scaling"
            ],
            "hyperparameterTuning": [
                "Regularization (C)"
            ],
            "dataPreprocessing": [
                "Handle class imbalance"
            ]
        }
    }
}