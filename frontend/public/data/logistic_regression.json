{
    "id": "logistic_regression",
    "name": "Logistic Regression",
    "category": "Supervised Learning - Classification",
    "difficulty": "Beginner",
    "estimatedTime": "50 minutes",
    "sections": {
        "introduction": {
            "title": "Introduction to Logistic Regression",
            "plainLanguage": "Logistic Regression is used when you need to predict yes/no, true/false, or categorical outcomes. Despite its name, it's a classification algorithm, not regression. It predicts the probability that something belongs to a particular category.",
            "realWorldAnalogy": "Think of a spam filter: given an email's features (words, sender, links), Logistic Regression calculates the probability it's spam. If probability > 50%, classify as spam; otherwise, not spam.",
            "whereAndWhy": "Used for binary classification: disease diagnosis (sick/healthy), customer churn (stay/leave), loan approval (approve/reject), email spam detection.",
            "learningType": "Supervised Learning (Classification)",
            "strengths": [
                "Outputs probabilities, not just class labelsâ€”tells you how confident the prediction is",
                "Simple, fast, and interpretable",
                "Works well with linearly separable data",
                "Less prone to overfitting than complex models"
            ],
            "limitations": [
                "Assumes linear decision boundary (can't handle complex patterns)",
                "Struggles with non-linear relationships",
                "Requires large sample size for stable estimates",
                "Sensitive to outliers"
            ]
        },
        "mathematical_model": {
            "title": "Mathematical Formulation",
            "introduction": "Logistic Regression uses the sigmoid function to map any real number to a probability between 0 and 1.",
            "equations": [
                {
                    "name": "Sigmoid Function",
                    "latex": "\\sigma(z) = \\frac{1}{1 + e^{-z}}",
                    "explanation": "Squashes any input z into range (0, 1). This gives us probabilities."
                },
                {
                    "name": "Hypothesis Function",
                    "latex": "h(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n)}}",
                    "explanation": "Predicts probability that y=1 given input x. Î¸ are parameters we learn."
                },
                {
                    "name": "Cost Function (Log Loss)",
                    "latex": "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1-h(x^{(i)}))]",
                    "explanation": "Penalizes wrong predictions heavily. Minimizing this makes predictions more accurate."
                },
                {
                    "name": "Gradient Descent Update",
                    "latex": "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)}",
                    "explanation": "Same form as linear regression, but h(x) is now the sigmoid function."
                }
            ],
            "keyTerms": {
                "Ïƒ(z)": "Sigmoid functionâ€”converts any number to probability",
                "Î¸": "Parameters (weights) we learn",
                "h(x)": "Predicted probability that y=1",
                "y": "Actual class label (0 or 1)",
                "Log Loss": "Cost function for classification"
            },
            "intuition": "The sigmoid function is like a smooth step: very negative inputs â†’ probability near 0; very positive inputs â†’ probability near 1. The model learns weights that push correct examples toward their true class."
        },
        "sample_io": "ðŸ“Š SAMPLE INPUT & OUTPUT\n\nðŸ“š Example: Predicting Exam Pass/Fail\n\nINPUT DATA (Training Examples):\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nHours Studied  â†’  Result\n      1        â†’  Fail (0)\n      2        â†’  Fail (0)\n      3        â†’  Fail (0)\n      4        â†’  Pass (1)\n      5        â†’  Pass (1)\n      6        â†’  Pass (1)\n\nâš™ï¸ WHAT THE MODEL LEARNS:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nThe algorithm finds an S-shaped curve (sigmoid function)\nProbability = Ïƒ(-3.5 + 1.2 Ã— Hours)\n\nâ€¢ Intercept (Î¸â‚€) = -3.5\nâ€¢ Weight (Î¸â‚) = 1.2\nâ€¢ Decision Boundary: ~3.5 hours (50% probability)\n\nðŸ“ˆ PREDICTIONS:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nHours  |  Probability  |  Prediction  |  Actual\n  1    |     5%        |    Fail      |   Fail  âœ“\n  2    |    12%        |    Fail      |   Fail  âœ“\n  3    |    27%        |    Fail      |   Fail  âœ“\n  4    |    50%        |    Pass      |   Pass  âœ“\n  5    |    82%        |    Pass      |   Pass  âœ“\n  6    |    95%        |    Pass      |   Pass  âœ“\n\nâœ… MODEL PERFORMANCE:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nâ€¢ Accuracy: 100% (All predictions correct!)\nâ€¢ Precision: 100%\nâ€¢ Recall: 100%\n\nðŸ’¡ EXAMPLE PREDICTIONS:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nQ: Will a student who studies 2 hours pass?\nA: Probability = 12% â†’ Prediction: Fail (Low confidence)\n\nQ: Will a student who studies 5 hours pass?\nA: Probability = 82% â†’ Prediction: Pass (High confidence)\n\nðŸŽ¯ KEY TAKEAWAY:\nLogistic Regression predicts probabilities between 0 and 1. If probability > 50%, we predict Pass; otherwise Fail. The S-shaped curve shows how probability smoothly increases with study hours!",
        "implementation_scratch": {
            "title": "Python Implementation (From Scratch)",
            "description": "Building Logistic Regression using only NumPy.",
            "code": "import numpy as np\n\nclass LogisticRegressionFromScratch:\n    def __init__(self, learning_rate=0.01, iterations=1000):\n        self.lr = learning_rate\n        self.iterations = iterations\n        self.theta = None\n    \n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def fit(self, X, y):\n        m, n = X.shape\n        X = np.c_[np.ones(m), X]\n        self.theta = np.zeros(n + 1)\n        for _ in range(self.iterations):\n            h = self.sigmoid(X @ self.theta)\n            gradient = (1/m) * X.T @ (h - y)\n            self.theta -= self.lr * gradient\n        return self\n\n    def predict(self, X):\n        m = X.shape[0]\n        X = np.c_[np.ones(m), X]\n        return (self.sigmoid(X @ self.theta) >= 0.5).astype(int)"
        },
        "implementation_api": {
            "title": "Python Implementation (Using scikit-learn)",
            "description": "Production-ready implementation.",
            "code": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X, y)\npreds = model.predict(X)"
        },
        "evaluation": {
            "title": "Model Evaluation",
            "why": "Accuracy alone is misleading for imbalanced datasets.",
            "metrics": [
                {
                    "name": "Accuracy",
                    "formula": "Accuracy = (TP + TN) / Total",
                    "interpretation": "Percentage of correct predictions.",
                    "example": "95% accuracy"
                }
            ]
        },
        "improvements": {
            "title": "Ways to Improve Performance",
            "featureEngineering": [
                "Add polynomial features",
                "Feature scaling"
            ],
            "hyperparameterTuning": [
                "Regularization (C)"
            ],
            "dataPreprocessing": [
                "Handle class imbalance"
            ]
        }
    }
}