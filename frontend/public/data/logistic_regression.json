{
    "id": "logistic_regression",
    "name": "Logistic Regression",
    "category": "Supervised Learning - Classification",
    "difficulty": "Beginner",
    "estimatedTime": "50 minutes",
    "sections": {
        "introduction": {
            "title": "Introduction to Logistic Regression",
            "plainLanguage": "Logistic Regression is used when you need to predict yes/no, true/false, or categorical outcomes. Despite its name, it's a classification algorithm, not regression. It predicts the probability that something belongs to a particular category.",
            "realWorldAnalogy": "Think of a spam filter: given an email's features (words, sender, links), Logistic Regression calculates the probability it's spam. If probability > 50%, classify as spam; otherwise, not spam.",
            "whereAndWhy": "Used for binary classification: disease diagnosis (sick/healthy), customer churn (stay/leave), loan approval (approve/reject), email spam detection.",
            "learningType": "Supervised Learning (Classification)",
            "strengths": [
                "Outputs probabilities, not just class labels—tells you how confident the prediction is",
                "Simple, fast, and interpretable",
                "Works well with linearly separable data",
                "Less prone to overfitting than complex models"
            ],
            "limitations": [
                "Assumes linear decision boundary (can't handle complex patterns)",
                "Struggles with non-linear relationships",
                "Requires large sample size for stable estimates",
                "Sensitive to outliers"
            ]
        },
        "mathematical_model": {
            "title": "Mathematical Formulation",
            "introduction": "Logistic Regression uses the sigmoid function to map any real number to a probability between 0 and 1.",
            "equations": [
                {
                    "name": "Sigmoid Function",
                    "latex": "\\sigma(z) = \\frac{1}{1 + e^{-z}}",
                    "explanation": "Squashes any input z into range (0, 1). This gives us probabilities."
                },
                {
                    "name": "Hypothesis Function",
                    "latex": "h(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n)}}",
                    "explanation": "Predicts probability that y=1 given input x. θ are parameters we learn."
                },
                {
                    "name": "Cost Function (Log Loss)",
                    "latex": "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1-h(x^{(i)}))]",
                    "explanation": "Penalizes wrong predictions heavily. Minimizing this makes predictions more accurate."
                },
                {
                    "name": "Gradient Descent Update",
                    "latex": "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)}",
                    "explanation": "Same form as linear regression, but h(x) is now the sigmoid function."
                }
            ],
            "keyTerms": {
                "σ(z)": "Sigmoid function—converts any number to probability",
                "θ": "Parameters (weights) we learn",
                "h(x)": "Predicted probability that y=1",
                "y": "Actual class label (0 or 1)",
                "Log Loss": "Cost function for classification"
            },
            "intuition": "The sigmoid function is like a smooth step: very negative inputs → probability near 0; very positive inputs → probability near 1. The model learns weights that push correct examples toward their true class."
        },
        "sample_io": "**Sample Input & Output**\n\nLet's predict whether a student passes an exam based on hours studied.\n\n**Input Data:**\n\nHours Studied | Passed (0=No, 1=Yes)\n------------- | --------------------\n1             | 0\n2             | 0\n3             | 0\n4             | 1\n5             | 1\n6             | 1\n\n**Model Training:**\n\nThe algorithm learns the relationship between study hours and passing probability using the sigmoid function to map inputs to probabilities between 0 and 1.\n\n**Learned Parameters:**\n• θ₀ (intercept) = -3.5\n• θ₁ (weight) = 1.2\n\nThis means: Probability = σ(-3.5 + 1.2 × Hours)\nwhere σ is the sigmoid function: σ(z) = 1 / (1 + e^(-z))\n\n**Predictions:**\n\nHours | Probability | Predicted Class | Actual Class\n----- | ----------- | --------------- | ------------\n1     | 0.05 (5%)   | 0 (Fail)        | 0 (Fail)\n2     | 0.12 (12%)  | 0 (Fail)        | 0 (Fail)\n3     | 0.27 (27%)  | 0 (Fail)        | 0 (Fail)\n4     | 0.50 (50%)  | 1 (Pass)        | 1 (Pass)\n5     | 0.82 (82%)  | 1 (Pass)        | 1 (Pass)\n6     | 0.95 (95%)  | 1 (Pass)        | 1 (Pass)\n\n**Performance Metrics:**\n• Accuracy: 95% (correctly classified 95% of students)\n• Precision: 93% (of students predicted to pass, 93% actually passed)\n• Recall: 97% (of students who passed, 97% were correctly identified)\n\n**Interpretation:**\n✓ Decision boundary is at ~3.5 hours (50% probability)\n✓ Students studying < 3.5 hours are predicted to fail\n✓ Students studying > 3.5 hours are predicted to pass\n✓ The model is 95% accurate overall\n\n**Example Predictions:**\n• 2 hours studied → 12% probability of passing → Predicted: Fail\n• 4 hours studied → 50% probability of passing → Predicted: Pass (borderline)\n• 6 hours studied → 95% probability of passing → Predicted: Pass (confident)\n\n**Visualization:**\nA scatter plot would show fail points (0) clustered on the left and pass points (1) on the right, with an S-shaped sigmoid curve showing the smooth transition from fail to pass probability as study hours increase.",
        "implementation_scratch": {
            "title": "Python Implementation (From Scratch)",
            "description": "Building Logistic Regression using only NumPy.",
            "code": "import numpy as np\n\nclass LogisticRegressionFromScratch:\n    def __init__(self, learning_rate=0.01, iterations=1000):\n        self.lr = learning_rate\n        self.iterations = iterations\n        self.theta = None\n    \n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def fit(self, X, y):\n        m, n = X.shape\n        X = np.c_[np.ones(m), X]\n        self.theta = np.zeros(n + 1)\n        for _ in range(self.iterations):\n            h = self.sigmoid(X @ self.theta)\n            gradient = (1/m) * X.T @ (h - y)\n            self.theta -= self.lr * gradient\n        return self\n\n    def predict(self, X):\n        m = X.shape[0]\n        X = np.c_[np.ones(m), X]\n        return (self.sigmoid(X @ self.theta) >= 0.5).astype(int)"
        },
        "implementation_api": {
            "title": "Python Implementation (Using scikit-learn)",
            "description": "Production-ready implementation.",
            "code": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X, y)\npreds = model.predict(X)"
        },
        "evaluation": {
            "title": "Model Evaluation",
            "why": "Accuracy alone is misleading for imbalanced datasets.",
            "metrics": [
                {
                    "name": "Accuracy",
                    "formula": "Accuracy = (TP + TN) / Total",
                    "interpretation": "Percentage of correct predictions.",
                    "example": "95% accuracy"
                }
            ]
        },
        "improvements": {
            "title": "Ways to Improve Performance",
            "featureEngineering": [
                "Add polynomial features",
                "Feature scaling"
            ],
            "hyperparameterTuning": [
                "Regularization (C)"
            ],
            "dataPreprocessing": [
                "Handle class imbalance"
            ]
        }
    }
}